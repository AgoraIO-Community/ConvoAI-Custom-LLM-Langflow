{
  "name": "agora-convo-ai-custom-llm-express",
  "version": "1.0.0",
  "description": "A node based service layer that accepts incoming requests from the Agora Convo AI service and passes it to an AI model, allowing for RAG and tools",
  "main": "src/server.ts",
  "scripts": {
    "start": "node dist/server.js",
    "start:docker": "docker compose up --build",
    "start:docker:detached": "docker compose up --build -d",
    "stop:docker": "docker compose down",
    "dev": "nodemon",
    "build": "tsc",
    "build:docker": "npm ci && npm run build && npm prune --production",
    "postinstall": "npm run build",
    "test": "echo \"Error: no test specified\" && exit 1",
    "logs": "docker compose logs -f",
    "clean": "rm -rf dist node_modules",
    "clean:docker": "docker compose down -v --rmi all"
  },
  "engines": {
    "node": ">=18.0.0"
  },
  "author": "digitallysavvy",
  "license": "MIT",
  "dependencies": {
    "@datastax/langflow-client": "^0.3.0",
    "@types/cors": "^2.8.17",
    "@types/express": "^4.17.21",
    "@types/morgan": "^1.9.9",
    "axios": "^1.6.2",
    "cors": "^2.8.5",
    "dotenv": "^16.3.1",
    "express": "^4.18.2",
    "helmet": "^7.1.0",
    "morgan": "^1.10.0",
    "openai": "^4.20.0",
    "serverless-http": "^3.2.0"
  },
  "devDependencies": {
    "nodemon": "^3.1.0",
    "ts-node": "^10.9.2",
    "typescript": "^5.7.3"
  }
}
